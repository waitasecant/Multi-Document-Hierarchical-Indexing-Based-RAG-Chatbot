{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from anytree import Node, RenderTree\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_chapter(i):\n",
    "    x = i.split('\\n')\n",
    "    return x[1].split(' ')[-1], x[-1]\n",
    "\n",
    "def display_tree(root):\n",
    "    for pre, _, node in RenderTree(root):\n",
    "        print(\"%s%s\" % (pre, node.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data//UnderstandingDeepLearning.pdf\"\n",
    "\n",
    "pattern_chap = r'\\nChapter \\d+\\n[^\\n]*'\n",
    "pattern_sec = r'\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])'\n",
    "pattern_seubsec = r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*'\n",
    "pattern_sec_subsec = re.compile(r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*|\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])')\n",
    "\n",
    "matches_chap = []\n",
    "matches_sec = []\n",
    "matches_subsec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(path)\n",
    "\n",
    "matches_chap.extend(re.findall(pattern_chap, text))\n",
    "matches_chap = [extract_chapter(i) for i in matches_chap]\n",
    "\n",
    "matches_sec.extend(re.findall(pattern_sec, text))\n",
    "matches_sec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_sec]\n",
    "\n",
    "matches_subsec.extend(re.findall(pattern_seubsec, text))\n",
    "matches_subsec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_subsec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_index():\n",
    "\n",
    "    root = Node(\"Understanding Deep Learning\")\n",
    "\n",
    "    # Joining chapters to root\n",
    "    for i in matches_chap:\n",
    "        Node(i[1], parent=root, identifier=i[0], text=None)\n",
    "\n",
    "    # Joining sections to chapters\n",
    "    for i in matches_sec:\n",
    "        for j in [(chap.identifier, chap) for chap in root.children]:\n",
    "            if i[0].split('.')[0] == j[0]:\n",
    "                Node(i[1], parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Joining subsections to sections\n",
    "    for i in matches_subsec:\n",
    "        all_sections = [i.children for i in root.children]\n",
    "        all_sections = [node for sublist in all_sections for node in sublist]\n",
    "        all_sections = [(sec.identifier,sec) for sec in all_sections]\n",
    "\n",
    "        for j in all_sections:\n",
    "            if '.'.join(i[0].split('.')[:-1]) == j[0]:\n",
    "                Node(i[1], parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Adding text between sections and subsections to subsections node\n",
    "    matches = list(re.finditer(pattern_sec_subsec, text))\n",
    "\n",
    "    for i in range(len(matches) - 1):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start()\n",
    "        subsection_num = matches[i].group().split('\\n')[1]\n",
    "        between_text = text[start:end]\n",
    "        between_text = ' '.join(between_text.split('\\n'))\n",
    "\n",
    "        leaves = [(l.identifier,l) for l in root.leaves]\n",
    "        for j in leaves:\n",
    "            if subsection_num == j[0]:\n",
    "                j[1].text = between_text\n",
    "    if matches:\n",
    "        between_text = ' '.join(text[matches[-1].start():matches[-1].start()+1000].split('\\n'))\n",
    "        root.leaves[-1].text = between_text\n",
    "        \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "htree = hierarchical_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Deep Learning\n",
      "├── Introduction\n",
      "│   ├── Supervised learning\n",
      "│   │   ├── Regression and classification problems\n",
      "│   │   ├── Inputs\n",
      "│   │   ├── Machine learning models\n",
      "│   │   ├── Deep neural networks\n",
      "│   │   └── Structured outputs\n",
      "│   ├── Unsupervised learning\n",
      "│   │   ├── Generative models\n",
      "│   │   ├── Latent variables\n",
      "│   │   └── Connecting supervised and unsupervised learning\n",
      "│   ├── Reinforcement learning\n",
      "│   │   └── Two examples\n",
      "│   ├── Ethics\n",
      "│   ├── Structure of book\n",
      "│   ├── Other books\n",
      "│   └── How to read this book\n",
      "├── Supervised learning\n",
      "│   ├── Supervised learning overview\n",
      "│   ├── Linear regression example\n",
      "│   │   ├── 1D linear regression model\n",
      "│   │   ├── Loss\n",
      "│   │   ├── Training\n",
      "│   │   └── Testing\n",
      "│   └── Summary\n",
      "├── Shallow neural networks\n",
      "│   ├── Neural network example\n",
      "│   │   ├── Neural network intuition\n",
      "│   │   └── Depicting neural networks\n",
      "│   ├── Universal approximation theorem\n",
      "│   ├── Multivariate inputs and outputs\n",
      "│   │   ├── Visualizing multivariate outputs\n",
      "│   │   └── Visualizing multivariate inputs\n",
      "│   ├── Shallow neural networks: general case\n",
      "│   ├── Terminology\n",
      "│   └── Summary\n",
      "├── Deep neural networks\n",
      "│   ├── Composing neural networks\n",
      "│   ├── From composing networks to deep networks\n",
      "│   ├── Deep neural networks\n",
      "│   │   └── Hyperparameters\n",
      "│   ├── Matrix notation\n",
      "│   │   └── General formulation\n",
      "│   ├── Shallow vs. deep neural networks\n",
      "│   │   ├── Ability to approximate different functions\n",
      "│   │   ├── Number of linear regions per parameter\n",
      "│   │   ├── Depth eﬀiciency\n",
      "│   │   ├── Large, structured inputs\n",
      "│   │   └── Training and generalization\n",
      "│   └── Summary\n",
      "├── Loss functions\n",
      "│   ├── Maximum likelihood\n",
      "│   │   ├── Computing a distribution over outputs\n",
      "│   │   ├── Maximum likelihood criterion\n",
      "│   │   ├── Maximizing log-likelihood\n",
      "│   │   ├── Minimizing negative log-likelihood\n",
      "│   │   └── Inference\n",
      "│   ├── Recipe for constructing loss functions\n",
      "│   ├── Multiple outputs\n",
      "│   ├── Cross-entropy loss\n",
      "│   └── Summary\n",
      "├── Fitting models\n",
      "│   ├── Gradient descent\n",
      "│   │   ├── Linear regression example\n",
      "│   │   ├── Gabor model example\n",
      "│   │   └── Local minima and saddle points\n",
      "│   ├── Stochastic gradient descent\n",
      "│   │   ├── Batches and epochs\n",
      "│   │   └── Properties of stochastic gradient descent\n",
      "│   ├── Momentum\n",
      "│   │   └── Nesterov accelerated momentum\n",
      "│   ├── Adam\n",
      "│   ├── Training algorithm hyperparameters\n",
      "│   └── Summary\n",
      "├── Gradients and initialization\n",
      "│   ├── Problem definitions\n",
      "│   ├── Computing derivatives\n",
      "│   ├── Toy example\n",
      "│   ├── Backpropagation algorithm\n",
      "│   │   ├── Backpropagation algorithm summary\n",
      "│   │   ├── Algorithmic differentiation\n",
      "│   │   └── Extension to arbitrary computational graphs\n",
      "│   ├── Parameter initialization\n",
      "│   │   ├── Initialization for forward pass\n",
      "│   │   ├── Initialization for backward pass\n",
      "│   │   └── Initialization for both forward and backward pass\n",
      "│   ├── Example training code\n",
      "│   └── Summary\n",
      "├── Measuring performance\n",
      "│   ├── Training a simple model\n",
      "│   ├── Sources of error\n",
      "│   │   ├── Noise, bias, and variance\n",
      "│   │   └── Mathematical formulation of test error\n",
      "│   ├── Reducing error\n",
      "│   │   ├── Reducing variance\n",
      "│   │   ├── Reducing bias\n",
      "│   │   └── Bias-variance trade-off\n",
      "│   ├── Double descent\n",
      "│   │   └── Explanation\n",
      "│   ├── Choosing hyperparameters\n",
      "│   └── Summary\n",
      "├── Regularization\n",
      "│   ├── Explicit regularization\n",
      "│   │   ├── Probabilistic interpretation\n",
      "│   │   └── L2 regularization\n",
      "│   ├── Implicit regularization\n",
      "│   │   ├── Implicit regularization in gradient descent\n",
      "│   │   └── Implicit regularization in stochastic gradient descent\n",
      "│   ├── Heuristics to improve performance\n",
      "│   │   ├── Early stopping\n",
      "│   │   ├── Ensembling\n",
      "│   │   ├── Dropout\n",
      "│   │   ├── Applying noise\n",
      "│   │   ├── Bayesian inference\n",
      "│   │   ├── Transfer learning and multi-task learning\n",
      "│   │   ├── Self-supervised learning\n",
      "│   │   └── Augmentation\n",
      "│   └── Summary\n",
      "├── Convolutional networks\n",
      "│   ├── Invariance and equivariance\n",
      "│   ├── Downsampling and upsampling\n",
      "│   │   ├── Downsampling\n",
      "│   │   ├── Upsampling\n",
      "│   │   └── Changing the number of channels\n",
      "│   ├── Applications\n",
      "│   │   ├── Image classification\n",
      "│   │   ├── Object detection\n",
      "│   │   └── Semantic segmentation\n",
      "│   └── Summary\n",
      "├── Residual networks\n",
      "│   ├── Sequential processing\n",
      "│   │   └── Limitations of sequential processing\n",
      "│   ├── Residual connections and residual blocks\n",
      "│   │   ├── Order of operations in residual blocks\n",
      "│   │   └── Deeper networks with residual connections\n",
      "│   ├── Exploding gradients in residual networks\n",
      "│   ├── Batch normalization\n",
      "│   │   └── Costs and benefits of batch normalization\n",
      "│   ├── Common residual architectures\n",
      "│   │   ├── ResNet\n",
      "│   │   ├── DenseNet\n",
      "│   │   └── U-Nets and hourglass networks\n",
      "│   ├── Why do nets with residual connections perform so well?\n",
      "│   └── Summary\n",
      "├── Transformers\n",
      "│   ├── Processing text data\n",
      "│   ├── Dot-product self-attention\n",
      "│   │   ├── Computing and weighting values\n",
      "│   │   ├── Computing attention weights\n",
      "│   │   ├── Self-attention summary\n",
      "│   │   └── Matrix form\n",
      "│   ├── Extensions to dot-product self-attention\n",
      "│   │   ├── Positional encoding\n",
      "│   │   ├── Scaled dot product self-attention\n",
      "│   │   └── Multiple heads\n",
      "│   ├── Transformers\n",
      "│   ├── Transformers for natural language processing\n",
      "│   │   ├── Tokenization\n",
      "│   │   ├── Embeddings\n",
      "│   │   └── Transformer model\n",
      "│   ├── Encoder model example: BERT\n",
      "│   │   ├── Pre-training\n",
      "│   │   └── Fine-tuning\n",
      "│   ├── Encoder-decoder model example: machine translation\n",
      "│   └── Transformers for long sequences\n",
      "├── Graph neural networks\n",
      "│   ├── What is a graph?\n",
      "│   │   └── Types of graphs\n",
      "│   ├── Graph representation\n",
      "│   │   ├── Properties of the adjacency matrix\n",
      "│   │   └── Permutation of node indices\n",
      "│   ├── Graph neural networks, tasks, and loss functions\n",
      "│   │   └── Tasks and loss functions\n",
      "│   ├── Graph convolutional networks\n",
      "│   │   ├── Equivariance and invariance\n",
      "│   │   ├── Parameter sharing\n",
      "│   │   └── Example GCN layer\n",
      "│   ├── Example: graph classification\n",
      "│   │   └── Training with batches\n",
      "│   ├── Inductive vs. transductive models\n",
      "│   ├── Example: node classification\n",
      "│   │   └── Choosing batches\n",
      "│   ├── Layers for graph convolutional networks\n",
      "│   │   ├── Combining current node and aggregated neighbors\n",
      "│   │   ├── Residual connections\n",
      "│   │   ├── Mean aggregation\n",
      "│   │   ├── Kipf normalization\n",
      "│   │   ├── Max pooling aggregation\n",
      "│   │   └── Aggregation by attention\n",
      "│   └── Edge graphs\n",
      "├── Unsupervised learning\n",
      "│   ├── Taxonomy of unsupervised learning models\n",
      "│   ├── What makes a good generative model?\n",
      "│   ├── Quantifying performance\n",
      "│   └── Summary\n",
      "├── Generative Adversarial Networks\n",
      "│   ├── Discrimination as a signal\n",
      "│   │   ├── GAN loss function\n",
      "│   │   ├── Training GANs\n",
      "│   │   ├── Deep convolutional GAN\n",
      "│   │   └── Diﬀiculty training GANs\n",
      "│   ├── Improving stability\n",
      "│   │   ├── Analysis of GAN loss function\n",
      "│   │   ├── Vanishing gradients\n",
      "│   │   ├── Wasserstein distance\n",
      "│   │   ├── Wasserstein distance for discrete distributions\n",
      "│   │   ├── Wasserstein distance for continuous distributions\n",
      "│   │   └── Wasserstein GAN loss function\n",
      "│   ├── Progressive growing, minibatch discrimination, and truncation\n",
      "│   ├── Conditional generation\n",
      "│   │   ├── Conditional GAN\n",
      "│   │   ├── Auxiliary classifier GAN\n",
      "│   │   └── InfoGAN\n",
      "│   ├── Image translation\n",
      "│   │   ├── Pix2Pix\n",
      "│   │   ├── Adversarial loss\n",
      "│   │   └── CycleGAN\n",
      "│   ├── StyleGAN\n",
      "│   └── Summary\n",
      "├── Normalizing flows\n",
      "│   ├── General case\n",
      "│   │   ├── Forward mapping with a deep neural network\n",
      "│   │   └── Desiderata for network layers\n",
      "│   ├── Invertible network layers\n",
      "│   │   ├── Linear flows\n",
      "│   │   ├── Elementwise flows\n",
      "│   │   ├── Coupling flows\n",
      "│   │   ├── Autoregressive flows\n",
      "│   │   ├── Inverse autoregressive flows\n",
      "│   │   ├── Residual flows: iRevNet\n",
      "│   │   └── Residual flows and contraction mappings: iResNet\n",
      "│   ├── Multi-scale flows\n",
      "│   ├── Applications\n",
      "│   │   ├── Modeling densities\n",
      "│   │   ├── Synthesis\n",
      "│   │   └── Approximating other density models\n",
      "│   └── Summary\n",
      "├── Variational autoencoders\n",
      "│   ├── Latent variable models\n",
      "│   │   └── Example: mixture of Gaussians\n",
      "│   ├── Nonlinear latent variable model\n",
      "│   │   └── Generation\n",
      "│   ├── Training\n",
      "│   │   ├── Evidence lower bound (ELBO)\n",
      "│   │   ├── Jensen’s inequality\n",
      "│   │   └── Deriving the bound\n",
      "│   ├── ELBO properties\n",
      "│   │   ├── Tightness of bound\n",
      "│   │   └── ELBO as reconstruction loss minus KL distance to prior\n",
      "│   ├── Variational approximation\n",
      "│   ├── The variational autoencoder\n",
      "│   │   └── VAE algorithm\n",
      "│   ├── The reparameterization trick\n",
      "│   ├── Applications\n",
      "│   │   ├── Approximating sample probability\n",
      "│   │   ├── Generation\n",
      "│   │   ├── Resynthesis\n",
      "│   │   └── Disentanglement\n",
      "│   └── Summary\n",
      "├── Diffusion models\n",
      "│   ├── Overview\n",
      "│   ├── Encoder (forward process)\n",
      "│   │   ├── Diffusion kernel q(zt|x)\n",
      "│   │   ├── Marginal distributions q(zt)\n",
      "│   │   ├── Conditional distribution q(zt−1|zt)\n",
      "│   │   └── Conditional diffusion distribution q(zt−1|zt, x)\n",
      "│   ├── Decoder model (reverse process)\n",
      "│   ├── Training\n",
      "│   │   ├── Evidence lower bound (ELBO)\n",
      "│   │   ├── Simplifying the ELBO\n",
      "│   │   ├── Analyzing the ELBO\n",
      "│   │   ├── Diffusion loss function\n",
      "│   │   └── Training procedure\n",
      "│   ├── Reparameterization of loss function\n",
      "│   │   ├── Reparameterization of target\n",
      "│   │   └── Reparameterization of network\n",
      "│   ├── Implementation\n",
      "│   │   ├── Application to images\n",
      "│   │   ├── Improving generation speed\n",
      "│   │   ├── Conditional generation\n",
      "│   │   └── Improving generation quality\n",
      "│   └── Summary\n",
      "├── Reinforcement learning\n",
      "│   ├── Markov decision processes, returns, and policies\n",
      "│   │   ├── Markov process\n",
      "│   │   ├── Markov reward process\n",
      "│   │   ├── Markov decision process\n",
      "│   │   ├── Partially observable Markov decision process\n",
      "│   │   └── Policy\n",
      "│   ├── Expected return\n",
      "│   │   ├── State and action values\n",
      "│   │   ├── Optimal policy\n",
      "│   │   └── Bellman equations\n",
      "│   ├── Tabular reinforcement learning\n",
      "│   │   ├── Dynamic programming\n",
      "│   │   ├── Monte Carlo methods\n",
      "│   │   └── Temporal difference methods\n",
      "│   ├── Fitted Q-learning\n",
      "│   │   ├── Deep Q-networks for playing ATARI games\n",
      "│   │   └── Double Q-learning and double deep Q-networks\n",
      "│   ├── Policy gradient methods\n",
      "│   │   ├── Derivation of gradient update\n",
      "│   │   ├── REINFORCE algorithm\n",
      "│   │   ├── Baselines\n",
      "│   │   └── State-dependent baselines\n",
      "│   ├── Actor-critic methods\n",
      "│   ├── Offline reinforcement learning\n",
      "│   └── Summary\n",
      "├── Why does deep learning work?\n",
      "│   ├── The case against deep learning\n",
      "│   │   ├── Training\n",
      "│   │   ├── Generalization\n",
      "│   │   └── The unreasonable effectiveness of deep learning\n",
      "│   ├── Factors that influence fitting performance\n",
      "│   │   ├── Dataset\n",
      "│   │   ├── Regularization\n",
      "│   │   ├── Stochastic training algorithms\n",
      "│   │   ├── Overparameterization\n",
      "│   │   ├── Activation functions\n",
      "│   │   ├── Initialization\n",
      "│   │   └── Network depth\n",
      "│   ├── Properties of loss functions\n",
      "│   │   ├── Multiple global minima\n",
      "│   │   ├── Route to the minimum\n",
      "│   │   ├── Connections between minima\n",
      "│   │   └── Curvature of loss surface\n",
      "│   ├── Factors that determine generalization\n",
      "│   │   ├── Training algorithms\n",
      "│   │   ├── Flatness of minimum\n",
      "│   │   ├── Architecture\n",
      "│   │   ├── Norm of weights\n",
      "│   │   ├── Overparameterization\n",
      "│   │   └── Leaving the data manifold\n",
      "│   ├── Do we need so many parameters?\n",
      "│   │   ├── Pruning\n",
      "│   │   ├── Knowledge distillation\n",
      "│   │   └── Discussion\n",
      "│   ├── Do networks have to be deep?\n",
      "│   │   ├── Complexity of modeled function\n",
      "│   │   ├── Tractability of training\n",
      "│   │   └── Inductive bias\n",
      "│   └── Summary\n",
      "└── Deep learning and ethics\n",
      "    ├── Value alignment\n",
      "    │   ├── Bias and fairness\n",
      "    │   ├── Artificial moral agency\n",
      "    │   ├── Transparency and opacity\n",
      "    │   └── Explainability and interpretability\n",
      "    ├── Intentional misuse\n",
      "    │   ├── Face recognition and analysis\n",
      "    │   ├── Militarization and political interference\n",
      "    │   ├── Fraud\n",
      "    │   └── Data privacy\n",
      "    ├── Other social, ethical, and professional issues\n",
      "    │   ├── Intellectual property\n",
      "    │   ├── Automation bias and moral deskilling\n",
      "    │   ├── Environmental impact\n",
      "    │   ├── Employment and society\n",
      "    │   └── Concentration of power\n",
      "    ├── Case study\n",
      "    ├── The value-free ideal of science\n",
      "    ├── Responsible AI research as a collective action problem\n",
      "    │   ├── Scientific communication\n",
      "    │   └── Diversity and heterogeneity\n",
      "    ├── Ways forward\n",
      "    └── Summary\n"
     ]
    }
   ],
   "source": [
    "display_tree(htree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database and table for storing the hierarchical index\n",
    "conn = sqlite3.connect('db//UnderstandingDeepLearning.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE IF NOT EXISTS understanding_deep_learning (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    parent_id INTEGER,\n",
    "    identifier TEXT,\n",
    "    title TEXT,\n",
    "    text TEXT,\n",
    "    FOREIGN KEY (parent_id) REFERENCES understanding_deep_learning (id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Recursively insert nodes into the database\n",
    "def tree_to_db(node, parent_id=None):\n",
    "    identifier = getattr(node, 'identifier', None)\n",
    "    title = node.name\n",
    "    text = getattr(node, 'text', None)\n",
    "    c.execute('INSERT INTO understanding_deep_learning (parent_id, identifier, title, text) VALUES (?, ?, ?, ?)', \n",
    "              (parent_id, identifier, title, text))\n",
    "    node_id = c.lastrowid\n",
    "    for child in node.children:\n",
    "        tree_to_db(child, node_id)\n",
    "\n",
    "tree_to_db(htree)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
