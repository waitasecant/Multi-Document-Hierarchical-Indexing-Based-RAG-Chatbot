{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing necessary libraries\n",
    "import re\n",
    "import fitz\n",
    "import nltk\n",
    "import bm25s\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "import transformers\n",
    "from nltk.corpus import wordnet\n",
    "from transformers import pipeline\n",
    "from nltk.stem import PorterStemmer\n",
    "from anytree import Node, RenderTree\n",
    "from gensim.utils import simple_preprocess\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_chapter(i):\n",
    "    x = i.split('\\n')\n",
    "    return x[1].split(' ')[-1], x[-1]\n",
    "\n",
    "def display_tree(root):\n",
    "    for pre, _, node in RenderTree(root):\n",
    "        print(\"%s%s\" % (pre, node.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data//UnderstandingDeepLearning.pdf\"\n",
    "\n",
    "pattern_chap = r'\\nChapter \\d+\\n[^\\n]*'\n",
    "pattern_sec = r'\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])'\n",
    "pattern_seubsec = r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*'\n",
    "pattern_sec_subsec = re.compile(r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*|\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])')\n",
    "\n",
    "matches_chap = []\n",
    "matches_sec = []\n",
    "matches_subsec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(path)\n",
    "\n",
    "matches_chap.extend(re.findall(pattern_chap, text))\n",
    "matches_chap = [extract_chapter(i) for i in matches_chap]\n",
    "\n",
    "matches_sec.extend(re.findall(pattern_sec, text))\n",
    "matches_sec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_sec]\n",
    "\n",
    "matches_subsec.extend(re.findall(pattern_seubsec, text))\n",
    "matches_subsec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_subsec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing hierarchical indexing to the document\n",
    "def hierarchical_index():\n",
    "\n",
    "    root = Node(\"Understanding Deep Learning\")\n",
    "\n",
    "    # Joining chapters to root\n",
    "    for i in matches_chap:\n",
    "        Node(i[1], parent=root, identifier=i[0], text=None)\n",
    "\n",
    "    # Joining sections to chapters\n",
    "    for i in matches_sec:\n",
    "        for j in [(chap.identifier, chap) for chap in root.children]:\n",
    "            if i[0].split('.')[0] == j[0]:\n",
    "                Node(i[1], parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Joining subsections to sections\n",
    "    for i in matches_subsec:\n",
    "        all_sections = [i.children for i in root.children]\n",
    "        all_sections = [node for sublist in all_sections for node in sublist]\n",
    "        all_sections = [(sec.identifier,sec) for sec in all_sections]\n",
    "\n",
    "        for j in all_sections:\n",
    "            if '.'.join(i[0].split('.')[:-1]) == j[0]:\n",
    "                Node(i[1], parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Adding text between sections and subsections to subsections node\n",
    "    matches = list(re.finditer(pattern_sec_subsec, text))\n",
    "\n",
    "    for i in range(len(matches) - 1):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start()\n",
    "        subsection_num = matches[i].group().split('\\n')[1]\n",
    "        between_text = text[start:end]\n",
    "        between_text = ' '.join(between_text.split('\\n'))\n",
    "\n",
    "        leaves = [(l.identifier,l) for l in root.leaves]\n",
    "        for j in leaves:\n",
    "            if subsection_num == j[0]:\n",
    "                j[1].text = between_text\n",
    "    if matches:\n",
    "        between_text = ' '.join(text[matches[-1].start():matches[-1].start()+1000].split('\\n'))\n",
    "        root.leaves[-1].text = between_text\n",
    "        \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Deep Learning\n",
      "├── Introduction\n",
      "│   ├── Supervised learning\n",
      "│   │   ├── Regression and classification problems\n",
      "│   │   ├── Inputs\n",
      "│   │   ├── Machine learning models\n",
      "│   │   ├── Deep neural networks\n",
      "│   │   └── Structured outputs\n",
      "│   ├── Unsupervised learning\n",
      "│   │   ├── Generative models\n",
      "│   │   ├── Latent variables\n",
      "│   │   └── Connecting supervised and unsupervised learning\n",
      "│   ├── Reinforcement learning\n",
      "│   │   └── Two examples\n",
      "│   ├── Ethics\n",
      "│   ├── Structure of book\n",
      "│   ├── Other books\n",
      "│   └── How to read this book\n",
      "├── Supervised learning\n",
      "│   ├── Supervised learning overview\n",
      "│   ├── Linear regression example\n",
      "│   │   ├── 1D linear regression model\n",
      "│   │   ├── Loss\n",
      "│   │   ├── Training\n",
      "│   │   └── Testing\n",
      "│   └── Summary\n",
      "├── Shallow neural networks\n",
      "│   ├── Neural network example\n",
      "│   │   ├── Neural network intuition\n",
      "│   │   └── Depicting neural networks\n",
      "│   ├── Universal approximation theorem\n",
      "│   ├── Multivariate inputs and outputs\n",
      "│   │   ├── Visualizing multivariate outputs\n",
      "│   │   └── Visualizing multivariate inputs\n",
      "│   ├── Shallow neural networks: general case\n",
      "│   ├── Terminology\n",
      "│   └── Summary\n",
      "├── Deep neural networks\n",
      "│   ├── Composing neural networks\n",
      "│   ├── From composing networks to deep networks\n",
      "│   ├── Deep neural networks\n",
      "│   │   └── Hyperparameters\n",
      "│   ├── Matrix notation\n",
      "│   │   └── General formulation\n",
      "│   ├── Shallow vs. deep neural networks\n",
      "│   │   ├── Ability to approximate different functions\n",
      "│   │   ├── Number of linear regions per parameter\n",
      "│   │   ├── Depth eﬀiciency\n",
      "│   │   ├── Large, structured inputs\n",
      "│   │   └── Training and generalization\n",
      "│   └── Summary\n",
      "├── Loss functions\n",
      "│   ├── Maximum likelihood\n",
      "│   │   ├── Computing a distribution over outputs\n",
      "│   │   ├── Maximum likelihood criterion\n",
      "│   │   ├── Maximizing log-likelihood\n",
      "│   │   ├── Minimizing negative log-likelihood\n",
      "│   │   └── Inference\n",
      "│   ├── Recipe for constructing loss functions\n",
      "│   ├── Multiple outputs\n",
      "│   ├── Cross-entropy loss\n",
      "│   └── Summary\n",
      "├── Fitting models\n",
      "│   ├── Gradient descent\n",
      "│   │   ├── Linear regression example\n",
      "│   │   ├── Gabor model example\n",
      "│   │   └── Local minima and saddle points\n",
      "│   ├── Stochastic gradient descent\n",
      "│   │   ├── Batches and epochs\n",
      "│   │   └── Properties of stochastic gradient descent\n",
      "│   ├── Momentum\n",
      "│   │   └── Nesterov accelerated momentum\n",
      "│   ├── Adam\n",
      "│   ├── Training algorithm hyperparameters\n",
      "│   └── Summary\n",
      "├── Gradients and initialization\n",
      "│   ├── Problem definitions\n",
      "│   ├── Computing derivatives\n",
      "│   ├── Toy example\n",
      "│   ├── Backpropagation algorithm\n",
      "│   │   ├── Backpropagation algorithm summary\n",
      "│   │   ├── Algorithmic differentiation\n",
      "│   │   └── Extension to arbitrary computational graphs\n",
      "│   ├── Parameter initialization\n",
      "│   │   ├── Initialization for forward pass\n",
      "│   │   ├── Initialization for backward pass\n",
      "│   │   └── Initialization for both forward and backward pass\n",
      "│   ├── Example training code\n",
      "│   └── Summary\n",
      "├── Measuring performance\n",
      "│   ├── Training a simple model\n",
      "│   ├── Sources of error\n",
      "│   │   ├── Noise, bias, and variance\n",
      "│   │   └── Mathematical formulation of test error\n",
      "│   ├── Reducing error\n",
      "│   │   ├── Reducing variance\n",
      "│   │   ├── Reducing bias\n",
      "│   │   └── Bias-variance trade-off\n",
      "│   ├── Double descent\n",
      "│   │   └── Explanation\n",
      "│   ├── Choosing hyperparameters\n",
      "│   └── Summary\n",
      "├── Regularization\n",
      "│   ├── Explicit regularization\n",
      "│   │   ├── Probabilistic interpretation\n",
      "│   │   └── L2 regularization\n",
      "│   ├── Implicit regularization\n",
      "│   │   ├── Implicit regularization in gradient descent\n",
      "│   │   └── Implicit regularization in stochastic gradient descent\n",
      "│   ├── Heuristics to improve performance\n",
      "│   │   ├── Early stopping\n",
      "│   │   ├── Ensembling\n",
      "│   │   ├── Dropout\n",
      "│   │   ├── Applying noise\n",
      "│   │   ├── Bayesian inference\n",
      "│   │   ├── Transfer learning and multi-task learning\n",
      "│   │   ├── Self-supervised learning\n",
      "│   │   └── Augmentation\n",
      "│   └── Summary\n",
      "├── Convolutional networks\n",
      "│   ├── Invariance and equivariance\n",
      "│   ├── Downsampling and upsampling\n",
      "│   │   ├── Downsampling\n",
      "│   │   ├── Upsampling\n",
      "│   │   └── Changing the number of channels\n",
      "│   ├── Applications\n",
      "│   │   ├── Image classification\n",
      "│   │   ├── Object detection\n",
      "│   │   └── Semantic segmentation\n",
      "│   └── Summary\n",
      "├── Residual networks\n",
      "│   ├── Sequential processing\n",
      "│   │   └── Limitations of sequential processing\n",
      "│   ├── Residual connections and residual blocks\n",
      "│   │   ├── Order of operations in residual blocks\n",
      "│   │   └── Deeper networks with residual connections\n",
      "│   ├── Exploding gradients in residual networks\n",
      "│   ├── Batch normalization\n",
      "│   │   └── Costs and benefits of batch normalization\n",
      "│   ├── Common residual architectures\n",
      "│   │   ├── ResNet\n",
      "│   │   ├── DenseNet\n",
      "│   │   └── U-Nets and hourglass networks\n",
      "│   ├── Why do nets with residual connections perform so well?\n",
      "│   └── Summary\n",
      "├── Transformers\n",
      "│   ├── Processing text data\n",
      "│   ├── Dot-product self-attention\n",
      "│   │   ├── Computing and weighting values\n",
      "│   │   ├── Computing attention weights\n",
      "│   │   ├── Self-attention summary\n",
      "│   │   └── Matrix form\n",
      "│   ├── Extensions to dot-product self-attention\n",
      "│   │   ├── Positional encoding\n",
      "│   │   ├── Scaled dot product self-attention\n",
      "│   │   └── Multiple heads\n",
      "│   ├── Transformers\n",
      "│   ├── Transformers for natural language processing\n",
      "│   │   ├── Tokenization\n",
      "│   │   ├── Embeddings\n",
      "│   │   └── Transformer model\n",
      "│   ├── Encoder model example: BERT\n",
      "│   │   ├── Pre-training\n",
      "│   │   └── Fine-tuning\n",
      "│   ├── Encoder-decoder model example: machine translation\n",
      "│   └── Transformers for long sequences\n",
      "├── Graph neural networks\n",
      "│   ├── What is a graph?\n",
      "│   │   └── Types of graphs\n",
      "│   ├── Graph representation\n",
      "│   │   ├── Properties of the adjacency matrix\n",
      "│   │   └── Permutation of node indices\n",
      "│   ├── Graph neural networks, tasks, and loss functions\n",
      "│   │   └── Tasks and loss functions\n",
      "│   ├── Graph convolutional networks\n",
      "│   │   ├── Equivariance and invariance\n",
      "│   │   ├── Parameter sharing\n",
      "│   │   └── Example GCN layer\n",
      "│   ├── Example: graph classification\n",
      "│   │   └── Training with batches\n",
      "│   ├── Inductive vs. transductive models\n",
      "│   ├── Example: node classification\n",
      "│   │   └── Choosing batches\n",
      "│   ├── Layers for graph convolutional networks\n",
      "│   │   ├── Combining current node and aggregated neighbors\n",
      "│   │   ├── Residual connections\n",
      "│   │   ├── Mean aggregation\n",
      "│   │   ├── Kipf normalization\n",
      "│   │   ├── Max pooling aggregation\n",
      "│   │   └── Aggregation by attention\n",
      "│   └── Edge graphs\n",
      "├── Unsupervised learning\n",
      "│   ├── Taxonomy of unsupervised learning models\n",
      "│   ├── What makes a good generative model?\n",
      "│   ├── Quantifying performance\n",
      "│   └── Summary\n",
      "├── Generative Adversarial Networks\n",
      "│   ├── Discrimination as a signal\n",
      "│   │   ├── GAN loss function\n",
      "│   │   ├── Training GANs\n",
      "│   │   ├── Deep convolutional GAN\n",
      "│   │   └── Diﬀiculty training GANs\n",
      "│   ├── Improving stability\n",
      "│   │   ├── Analysis of GAN loss function\n",
      "│   │   ├── Vanishing gradients\n",
      "│   │   ├── Wasserstein distance\n",
      "│   │   ├── Wasserstein distance for discrete distributions\n",
      "│   │   ├── Wasserstein distance for continuous distributions\n",
      "│   │   └── Wasserstein GAN loss function\n",
      "│   ├── Progressive growing, minibatch discrimination, and truncation\n",
      "│   ├── Conditional generation\n",
      "│   │   ├── Conditional GAN\n",
      "│   │   ├── Auxiliary classifier GAN\n",
      "│   │   └── InfoGAN\n",
      "│   ├── Image translation\n",
      "│   │   ├── Pix2Pix\n",
      "│   │   ├── Adversarial loss\n",
      "│   │   └── CycleGAN\n",
      "│   ├── StyleGAN\n",
      "│   └── Summary\n",
      "├── Normalizing flows\n",
      "│   ├── General case\n",
      "│   │   ├── Forward mapping with a deep neural network\n",
      "│   │   └── Desiderata for network layers\n",
      "│   ├── Invertible network layers\n",
      "│   │   ├── Linear flows\n",
      "│   │   ├── Elementwise flows\n",
      "│   │   ├── Coupling flows\n",
      "│   │   ├── Autoregressive flows\n",
      "│   │   ├── Inverse autoregressive flows\n",
      "│   │   ├── Residual flows: iRevNet\n",
      "│   │   └── Residual flows and contraction mappings: iResNet\n",
      "│   ├── Multi-scale flows\n",
      "│   ├── Applications\n",
      "│   │   ├── Modeling densities\n",
      "│   │   ├── Synthesis\n",
      "│   │   └── Approximating other density models\n",
      "│   └── Summary\n",
      "├── Variational autoencoders\n",
      "│   ├── Latent variable models\n",
      "│   │   └── Example: mixture of Gaussians\n",
      "│   ├── Nonlinear latent variable model\n",
      "│   │   └── Generation\n",
      "│   ├── Training\n",
      "│   │   ├── Evidence lower bound (ELBO)\n",
      "│   │   ├── Jensen’s inequality\n",
      "│   │   └── Deriving the bound\n",
      "│   ├── ELBO properties\n",
      "│   │   ├── Tightness of bound\n",
      "│   │   └── ELBO as reconstruction loss minus KL distance to prior\n",
      "│   ├── Variational approximation\n",
      "│   ├── The variational autoencoder\n",
      "│   │   └── VAE algorithm\n",
      "│   ├── The reparameterization trick\n",
      "│   ├── Applications\n",
      "│   │   ├── Approximating sample probability\n",
      "│   │   ├── Generation\n",
      "│   │   ├── Resynthesis\n",
      "│   │   └── Disentanglement\n",
      "│   └── Summary\n",
      "├── Diffusion models\n",
      "│   ├── Overview\n",
      "│   ├── Encoder (forward process)\n",
      "│   │   ├── Diffusion kernel q(zt|x)\n",
      "│   │   ├── Marginal distributions q(zt)\n",
      "│   │   ├── Conditional distribution q(zt−1|zt)\n",
      "│   │   └── Conditional diffusion distribution q(zt−1|zt, x)\n",
      "│   ├── Decoder model (reverse process)\n",
      "│   ├── Training\n",
      "│   │   ├── Evidence lower bound (ELBO)\n",
      "│   │   ├── Simplifying the ELBO\n",
      "│   │   ├── Analyzing the ELBO\n",
      "│   │   ├── Diffusion loss function\n",
      "│   │   └── Training procedure\n",
      "│   ├── Reparameterization of loss function\n",
      "│   │   ├── Reparameterization of target\n",
      "│   │   └── Reparameterization of network\n",
      "│   ├── Implementation\n",
      "│   │   ├── Application to images\n",
      "│   │   ├── Improving generation speed\n",
      "│   │   ├── Conditional generation\n",
      "│   │   └── Improving generation quality\n",
      "│   └── Summary\n",
      "├── Reinforcement learning\n",
      "│   ├── Markov decision processes, returns, and policies\n",
      "│   │   ├── Markov process\n",
      "│   │   ├── Markov reward process\n",
      "│   │   ├── Markov decision process\n",
      "│   │   ├── Partially observable Markov decision process\n",
      "│   │   └── Policy\n",
      "│   ├── Expected return\n",
      "│   │   ├── State and action values\n",
      "│   │   ├── Optimal policy\n",
      "│   │   └── Bellman equations\n",
      "│   ├── Tabular reinforcement learning\n",
      "│   │   ├── Dynamic programming\n",
      "│   │   ├── Monte Carlo methods\n",
      "│   │   └── Temporal difference methods\n",
      "│   ├── Fitted Q-learning\n",
      "│   │   ├── Deep Q-networks for playing ATARI games\n",
      "│   │   └── Double Q-learning and double deep Q-networks\n",
      "│   ├── Policy gradient methods\n",
      "│   │   ├── Derivation of gradient update\n",
      "│   │   ├── REINFORCE algorithm\n",
      "│   │   ├── Baselines\n",
      "│   │   └── State-dependent baselines\n",
      "│   ├── Actor-critic methods\n",
      "│   ├── Offline reinforcement learning\n",
      "│   └── Summary\n",
      "├── Why does deep learning work?\n",
      "│   ├── The case against deep learning\n",
      "│   │   ├── Training\n",
      "│   │   ├── Generalization\n",
      "│   │   └── The unreasonable effectiveness of deep learning\n",
      "│   ├── Factors that influence fitting performance\n",
      "│   │   ├── Dataset\n",
      "│   │   ├── Regularization\n",
      "│   │   ├── Stochastic training algorithms\n",
      "│   │   ├── Overparameterization\n",
      "│   │   ├── Activation functions\n",
      "│   │   ├── Initialization\n",
      "│   │   └── Network depth\n",
      "│   ├── Properties of loss functions\n",
      "│   │   ├── Multiple global minima\n",
      "│   │   ├── Route to the minimum\n",
      "│   │   ├── Connections between minima\n",
      "│   │   └── Curvature of loss surface\n",
      "│   ├── Factors that determine generalization\n",
      "│   │   ├── Training algorithms\n",
      "│   │   ├── Flatness of minimum\n",
      "│   │   ├── Architecture\n",
      "│   │   ├── Norm of weights\n",
      "│   │   ├── Overparameterization\n",
      "│   │   └── Leaving the data manifold\n",
      "│   ├── Do we need so many parameters?\n",
      "│   │   ├── Pruning\n",
      "│   │   ├── Knowledge distillation\n",
      "│   │   └── Discussion\n",
      "│   ├── Do networks have to be deep?\n",
      "│   │   ├── Complexity of modeled function\n",
      "│   │   ├── Tractability of training\n",
      "│   │   └── Inductive bias\n",
      "│   └── Summary\n",
      "└── Deep learning and ethics\n",
      "    ├── Value alignment\n",
      "    │   ├── Bias and fairness\n",
      "    │   ├── Artificial moral agency\n",
      "    │   ├── Transparency and opacity\n",
      "    │   └── Explainability and interpretability\n",
      "    ├── Intentional misuse\n",
      "    │   ├── Face recognition and analysis\n",
      "    │   ├── Militarization and political interference\n",
      "    │   ├── Fraud\n",
      "    │   └── Data privacy\n",
      "    ├── Other social, ethical, and professional issues\n",
      "    │   ├── Intellectual property\n",
      "    │   ├── Automation bias and moral deskilling\n",
      "    │   ├── Environmental impact\n",
      "    │   ├── Employment and society\n",
      "    │   └── Concentration of power\n",
      "    ├── Case study\n",
      "    ├── The value-free ideal of science\n",
      "    ├── Responsible AI research as a collective action problem\n",
      "    │   ├── Scientific communication\n",
      "    │   └── Diversity and heterogeneity\n",
      "    ├── Ways forward\n",
      "    └── Summary\n"
     ]
    }
   ],
   "source": [
    "htree = hierarchical_index()\n",
    "display_tree(htree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create SQLite database and table for storing the hierarchical index\n",
    "# conn = sqlite3.connect('db//UnderstandingDeepLearning.db')\n",
    "# c = conn.cursor()\n",
    "\n",
    "# c.execute('''\n",
    "# CREATE TABLE IF NOT EXISTS understanding_deep_learning (\n",
    "#     id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "#     parent_id INTEGER,\n",
    "#     identifier TEXT,\n",
    "#     title TEXT,\n",
    "#     text TEXT,\n",
    "#     FOREIGN KEY (parent_id) REFERENCES understanding_deep_learning (id)\n",
    "# )\n",
    "# ''')\n",
    "\n",
    "# # Recursively insert nodes into the database\n",
    "# def tree_to_db(node, parent_id=None):\n",
    "#     identifier = getattr(node, 'identifier', None)\n",
    "#     title = node.name\n",
    "#     text = getattr(node, 'text', None)\n",
    "#     c.execute('INSERT INTO understanding_deep_learning (parent_id, identifier, title, text) VALUES (?, ?, ?, ?)', \n",
    "#               (parent_id, identifier, title, text))\n",
    "#     node_id = c.lastrowid\n",
    "#     for child in node.children:\n",
    "#         tree_to_db(child, node_id)\n",
    "\n",
    "# tree_to_db(htree)\n",
    "\n",
    "# conn.commit()\n",
    "# conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus():\n",
    "    conn = sqlite3.connect('db//UnderstandingDeepLearning.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('SELECT id, title, text FROM understanding_deep_learning')\n",
    "    rows = c.fetchall()\n",
    "    conn.close()\n",
    "    corpus = []\n",
    "    doc_ids = []\n",
    "    for row in rows:\n",
    "        if row[2]:\n",
    "            corpus.append(row[2])\n",
    "            doc_ids.append(row[0])\n",
    "    return corpus, doc_ids\n",
    "\n",
    "def expand_query(query):\n",
    "    words = simple_preprocess(query)\n",
    "    expanded_query = set(words)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words:\n",
    "        # Add synonyms\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        for syn in synonyms:\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_query.add(lemma.name())\n",
    "        \n",
    "        # Add stemmed words\n",
    "        expanded_query.add(stemmer.stem(word))\n",
    "\n",
    "    return list(expanded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, doc_ids = get_corpus()\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)\n",
    "corpus_tokens = bm25s.tokenize(corpus, show_progress=False)\n",
    "retriever = bm25s.BM25(corpus=corpus)\n",
    "retriever.index(corpus_tokens, show_progress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(query, k=10):\n",
    "    expanded_query = expand_query(query)\n",
    "    bm25_results = retriever.get_scores(expanded_query)  \n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "    bert_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0].cpu().numpy()\n",
    "\n",
    "    combined_scores = []\n",
    "    for doc_id, bm25_score in zip(doc_ids, bm25_results):\n",
    "        bert_score = bert_scores[doc_ids.index(doc_id)]\n",
    "        combined_scores.append((doc_id, bm25_score + bert_score))\n",
    "    \n",
    "    combined_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_k_results = combined_scores[:k]\n",
    "\n",
    "    retrieved_docs = [(doc_id, corpus[doc_ids.index(doc_id)], score) for doc_id, score in top_k_results]\n",
    "    \n",
    "    return retrieved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"polynomial lower bound on width\"\n",
    "# results = retrieve_documents(query)\n",
    "# for doc_id, doc_text, score in results:\n",
    "#     print(f\"Document ID: {doc_id}, Score: {score}\\nText: {doc_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_system(query):\n",
    "    retrieved_docs = retrieve_documents(query)\n",
    "    combined_text = \" \".join([doc_text for _, doc_text, _ in retrieved_docs])\n",
    "    answer = qa_pipeline({'question': query, 'context': combined_text})\n",
    "    return answer['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: the distance between the aggregate posterior and the product of its marginals\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about latent variables\"\n",
    "answer = rag_system(query)\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
