{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from anytree import Node, RenderTree\n",
    "import re\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_chapter(i):\n",
    "    x = i.split('\\n')\n",
    "    return x[1].split(' ')[-1], x[-1]\n",
    "\n",
    "def display_tree(root):\n",
    "    for pre, _, node in RenderTree(root):\n",
    "        print(\"%s%s\" % (pre, node.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data//UnderstandingDeepLearning.pdf\"\n",
    "\n",
    "pattern_chap = r'\\nChapter \\d+\\n[^\\n]*'\n",
    "pattern_sec = r'\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])'\n",
    "pattern_seubsec = r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*'\n",
    "pattern_sec_subsec = re.compile(r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*|\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])')\n",
    "\n",
    "matches_chap = []\n",
    "matches_sec = []\n",
    "matches_subsec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(path)\n",
    "\n",
    "matches_chap.extend(re.findall(pattern_chap, text))\n",
    "matches_chap = [extract_chapter(i) for i in matches_chap]\n",
    "\n",
    "matches_sec.extend(re.findall(pattern_sec, text))\n",
    "matches_sec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_sec]\n",
    "\n",
    "matches_subsec.extend(re.findall(pattern_seubsec, text))\n",
    "matches_subsec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_subsec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_index():\n",
    "\n",
    "    root = Node(\"Understanding Deep Learning\")\n",
    "\n",
    "    # Joining chapters to root\n",
    "    for i in matches_chap:\n",
    "        Node(i, parent=root, identifier=i[0], text=None)\n",
    "\n",
    "    # Joining sections to chapters\n",
    "    for i in matches_sec:\n",
    "        for j in [(chap.name[0], chap) for chap in root.children]:\n",
    "            if i[0].split('.')[0] == j[0]:\n",
    "                Node(i, parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Joining subsections to sections\n",
    "    for i in matches_subsec:\n",
    "        all_sections = [i.children for i in root.children]\n",
    "        all_sections = [node for sublist in all_sections for node in sublist]\n",
    "        all_sections = [(sec.name[0],sec) for sec in all_sections]\n",
    "\n",
    "        for j in all_sections:\n",
    "            if '.'.join(i[0].split('.')[:-1]) == j[0]:\n",
    "                Node(i, parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Adding text between sections and subsections to subsections node\n",
    "    matches = list(re.finditer(pattern_sec_subsec, text))\n",
    "\n",
    "    for i in range(len(matches) - 1):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start()\n",
    "        subsection_num = matches[i].group().split('\\n')[1]\n",
    "        between_text = text[start:end]\n",
    "        between_text = ' '.join(between_text.split('\\n'))\n",
    "\n",
    "        leaves = [(l.name[0],l) for l in root.leaves]\n",
    "        for j in leaves:\n",
    "            if subsection_num == j[0]:\n",
    "                j[1].text = between_text\n",
    "                # Node(between_text, parent=j[1])\n",
    "    if matches:\n",
    "        between_text = ' '.join(text[matches[-1].start():matches[-1].start()+1000].split('\\n'))\n",
    "        root.leaves[-1].text = between_text\n",
    "        # Node(between_text,root.leaves[-1])\n",
    "        \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "htree = hierarchical_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Understanding Deep Learning\n",
      "├── ('1', 'Introduction')\n",
      "│   ├── ('1.1', 'Supervised learning')\n",
      "│   │   ├── ('1.1.1', 'Regression and classification problems')\n",
      "│   │   ├── ('1.1.2', 'Inputs')\n",
      "│   │   ├── ('1.1.3', 'Machine learning models')\n",
      "│   │   ├── ('1.1.4', 'Deep neural networks')\n",
      "│   │   └── ('1.1.5', 'Structured outputs')\n",
      "│   ├── ('1.2', 'Unsupervised learning')\n",
      "│   │   ├── ('1.2.1', 'Generative models')\n",
      "│   │   ├── ('1.2.2', 'Latent variables')\n",
      "│   │   └── ('1.2.3', 'Connecting supervised and unsupervised learning')\n",
      "│   ├── ('1.3', 'Reinforcement learning')\n",
      "│   │   └── ('1.3.1', 'Two examples')\n",
      "│   ├── ('1.4', 'Ethics')\n",
      "│   ├── ('1.5', 'Structure of book')\n",
      "│   ├── ('1.6', 'Other books')\n",
      "│   └── ('1.7', 'How to read this book')\n",
      "├── ('2', 'Supervised learning')\n",
      "│   ├── ('2.1', 'Supervised learning overview')\n",
      "│   ├── ('2.2', 'Linear regression example')\n",
      "│   │   ├── ('2.2.1', '1D linear regression model')\n",
      "│   │   ├── ('2.2.2', 'Loss')\n",
      "│   │   ├── ('2.2.3', 'Training')\n",
      "│   │   └── ('2.2.4', 'Testing')\n",
      "│   └── ('2.3', 'Summary')\n",
      "├── ('3', 'Shallow neural networks')\n",
      "│   ├── ('3.1', 'Neural network example')\n",
      "│   │   ├── ('3.1.1', 'Neural network intuition')\n",
      "│   │   └── ('3.1.2', 'Depicting neural networks')\n",
      "│   ├── ('3.2', 'Universal approximation theorem')\n",
      "│   ├── ('3.3', 'Multivariate inputs and outputs')\n",
      "│   │   ├── ('3.3.1', 'Visualizing multivariate outputs')\n",
      "│   │   └── ('3.3.2', 'Visualizing multivariate inputs')\n",
      "│   ├── ('3.4', 'Shallow neural networks: general case')\n",
      "│   ├── ('3.5', 'Terminology')\n",
      "│   └── ('3.6', 'Summary')\n",
      "├── ('4', 'Deep neural networks')\n",
      "│   ├── ('4.1', 'Composing neural networks')\n",
      "│   ├── ('4.2', 'From composing networks to deep networks')\n",
      "│   ├── ('4.3', 'Deep neural networks')\n",
      "│   │   └── ('4.3.1', 'Hyperparameters')\n",
      "│   ├── ('4.4', 'Matrix notation')\n",
      "│   │   └── ('4.4.1', 'General formulation')\n",
      "│   ├── ('4.5', 'Shallow vs. deep neural networks')\n",
      "│   │   ├── ('4.5.1', 'Ability to approximate different functions')\n",
      "│   │   ├── ('4.5.2', 'Number of linear regions per parameter')\n",
      "│   │   ├── ('4.5.3', 'Depth eﬀiciency')\n",
      "│   │   ├── ('4.5.4', 'Large, structured inputs')\n",
      "│   │   └── ('4.5.5', 'Training and generalization')\n",
      "│   └── ('4.6', 'Summary')\n",
      "├── ('5', 'Loss functions')\n",
      "│   ├── ('5.1', 'Maximum likelihood')\n",
      "│   │   ├── ('5.1.1', 'Computing a distribution over outputs')\n",
      "│   │   ├── ('5.1.2', 'Maximum likelihood criterion')\n",
      "│   │   ├── ('5.1.3', 'Maximizing log-likelihood')\n",
      "│   │   ├── ('5.1.4', 'Minimizing negative log-likelihood')\n",
      "│   │   └── ('5.1.5', 'Inference')\n",
      "│   ├── ('5.2', 'Recipe for constructing loss functions')\n",
      "│   ├── ('5.6', 'Multiple outputs')\n",
      "│   ├── ('5.7', 'Cross-entropy loss')\n",
      "│   └── ('5.8', 'Summary')\n",
      "├── ('6', 'Fitting models')\n",
      "│   ├── ('6.1', 'Gradient descent')\n",
      "│   │   ├── ('6.1.1', 'Linear regression example')\n",
      "│   │   ├── ('6.1.2', 'Gabor model example')\n",
      "│   │   └── ('6.1.3', 'Local minima and saddle points')\n",
      "│   ├── ('6.2', 'Stochastic gradient descent')\n",
      "│   │   ├── ('6.2.1', 'Batches and epochs')\n",
      "│   │   └── ('6.2.2', 'Properties of stochastic gradient descent')\n",
      "│   ├── ('6.3', 'Momentum')\n",
      "│   │   └── ('6.3.1', 'Nesterov accelerated momentum')\n",
      "│   ├── ('6.4', 'Adam')\n",
      "│   ├── ('6.5', 'Training algorithm hyperparameters')\n",
      "│   └── ('6.6', 'Summary')\n",
      "├── ('7', 'Gradients and initialization')\n",
      "│   ├── ('7.1', 'Problem definitions')\n",
      "│   ├── ('7.2', 'Computing derivatives')\n",
      "│   ├── ('7.3', 'Toy example')\n",
      "│   ├── ('7.4', 'Backpropagation algorithm')\n",
      "│   │   ├── ('7.4.1', 'Backpropagation algorithm summary')\n",
      "│   │   ├── ('7.4.2', 'Algorithmic differentiation')\n",
      "│   │   └── ('7.4.3', 'Extension to arbitrary computational graphs')\n",
      "│   ├── ('7.5', 'Parameter initialization')\n",
      "│   │   ├── ('7.5.1', 'Initialization for forward pass')\n",
      "│   │   ├── ('7.5.2', 'Initialization for backward pass')\n",
      "│   │   └── ('7.5.3', 'Initialization for both forward and backward pass')\n",
      "│   ├── ('7.6', 'Example training code')\n",
      "│   └── ('7.7', 'Summary')\n",
      "├── ('8', 'Measuring performance')\n",
      "│   ├── ('8.1', 'Training a simple model')\n",
      "│   ├── ('8.2', 'Sources of error')\n",
      "│   │   ├── ('8.2.1', 'Noise, bias, and variance')\n",
      "│   │   └── ('8.2.2', 'Mathematical formulation of test error')\n",
      "│   ├── ('8.3', 'Reducing error')\n",
      "│   │   ├── ('8.3.1', 'Reducing variance')\n",
      "│   │   ├── ('8.3.2', 'Reducing bias')\n",
      "│   │   └── ('8.3.3', 'Bias-variance trade-off')\n",
      "│   ├── ('8.4', 'Double descent')\n",
      "│   │   └── ('8.4.1', 'Explanation')\n",
      "│   ├── ('8.5', 'Choosing hyperparameters')\n",
      "│   └── ('8.6', 'Summary')\n",
      "├── ('9', 'Regularization')\n",
      "│   ├── ('9.1', 'Explicit regularization')\n",
      "│   │   ├── ('9.1.1', 'Probabilistic interpretation')\n",
      "│   │   └── ('9.1.2', 'L2 regularization')\n",
      "│   ├── ('9.2', 'Implicit regularization')\n",
      "│   │   ├── ('9.2.1', 'Implicit regularization in gradient descent')\n",
      "│   │   └── ('9.2.2', 'Implicit regularization in stochastic gradient descent')\n",
      "│   ├── ('9.3', 'Heuristics to improve performance')\n",
      "│   │   ├── ('9.3.1', 'Early stopping')\n",
      "│   │   ├── ('9.3.2', 'Ensembling')\n",
      "│   │   ├── ('9.3.3', 'Dropout')\n",
      "│   │   ├── ('9.3.4', 'Applying noise')\n",
      "│   │   ├── ('9.3.5', 'Bayesian inference')\n",
      "│   │   ├── ('9.3.6', 'Transfer learning and multi-task learning')\n",
      "│   │   ├── ('9.3.7', 'Self-supervised learning')\n",
      "│   │   └── ('9.3.8', 'Augmentation')\n",
      "│   └── ('9.4', 'Summary')\n",
      "├── ('10', 'Convolutional networks')\n",
      "│   ├── ('10.1', 'Invariance and equivariance')\n",
      "│   ├── ('10.4', 'Downsampling and upsampling')\n",
      "│   │   ├── ('10.4.1', 'Downsampling')\n",
      "│   │   ├── ('10.4.2', 'Upsampling')\n",
      "│   │   └── ('10.4.3', 'Changing the number of channels')\n",
      "│   ├── ('10.5', 'Applications')\n",
      "│   │   ├── ('10.5.1', 'Image classification')\n",
      "│   │   ├── ('10.5.2', 'Object detection')\n",
      "│   │   └── ('10.5.3', 'Semantic segmentation')\n",
      "│   └── ('10.6', 'Summary')\n",
      "├── ('11', 'Residual networks')\n",
      "│   ├── ('11.1', 'Sequential processing')\n",
      "│   │   └── ('11.1.1', 'Limitations of sequential processing')\n",
      "│   ├── ('11.2', 'Residual connections and residual blocks')\n",
      "│   │   ├── ('11.2.1', 'Order of operations in residual blocks')\n",
      "│   │   └── ('11.2.2', 'Deeper networks with residual connections')\n",
      "│   ├── ('11.3', 'Exploding gradients in residual networks')\n",
      "│   ├── ('11.4', 'Batch normalization')\n",
      "│   │   └── ('11.4.1', 'Costs and benefits of batch normalization')\n",
      "│   ├── ('11.5', 'Common residual architectures')\n",
      "│   │   ├── ('11.5.1', 'ResNet')\n",
      "│   │   ├── ('11.5.2', 'DenseNet')\n",
      "│   │   └── ('11.5.3', 'U-Nets and hourglass networks')\n",
      "│   ├── ('11.6', 'Why do nets with residual connections perform so well?')\n",
      "│   └── ('11.7', 'Summary')\n",
      "├── ('12', 'Transformers')\n",
      "│   ├── ('12.1', 'Processing text data')\n",
      "│   ├── ('12.2', 'Dot-product self-attention')\n",
      "│   │   ├── ('12.2.1', 'Computing and weighting values')\n",
      "│   │   ├── ('12.2.2', 'Computing attention weights')\n",
      "│   │   ├── ('12.2.3', 'Self-attention summary')\n",
      "│   │   └── ('12.2.4', 'Matrix form')\n",
      "│   ├── ('12.3', 'Extensions to dot-product self-attention')\n",
      "│   │   ├── ('12.3.1', 'Positional encoding')\n",
      "│   │   ├── ('12.3.2', 'Scaled dot product self-attention')\n",
      "│   │   └── ('12.3.3', 'Multiple heads')\n",
      "│   ├── ('12.4', 'Transformers')\n",
      "│   ├── ('12.5', 'Transformers for natural language processing')\n",
      "│   │   ├── ('12.5.1', 'Tokenization')\n",
      "│   │   ├── ('12.5.2', 'Embeddings')\n",
      "│   │   └── ('12.5.3', 'Transformer model')\n",
      "│   ├── ('12.6', 'Encoder model example: BERT')\n",
      "│   │   ├── ('12.6.1', 'Pre-training')\n",
      "│   │   └── ('12.6.2', 'Fine-tuning')\n",
      "│   ├── ('12.8', 'Encoder-decoder model example: machine translation')\n",
      "│   └── ('12.9', 'Transformers for long sequences')\n",
      "├── ('13', 'Graph neural networks')\n",
      "│   ├── ('13.1', 'What is a graph?')\n",
      "│   │   └── ('13.1.1', 'Types of graphs')\n",
      "│   ├── ('13.2', 'Graph representation')\n",
      "│   │   ├── ('13.2.1', 'Properties of the adjacency matrix')\n",
      "│   │   └── ('13.2.2', 'Permutation of node indices')\n",
      "│   ├── ('13.3', 'Graph neural networks, tasks, and loss functions')\n",
      "│   │   └── ('13.3.1', 'Tasks and loss functions')\n",
      "│   ├── ('13.4', 'Graph convolutional networks')\n",
      "│   │   ├── ('13.4.1', 'Equivariance and invariance')\n",
      "│   │   ├── ('13.4.2', 'Parameter sharing')\n",
      "│   │   └── ('13.4.3', 'Example GCN layer')\n",
      "│   ├── ('13.5', 'Example: graph classification')\n",
      "│   │   └── ('13.5.1', 'Training with batches')\n",
      "│   ├── ('13.6', 'Inductive vs. transductive models')\n",
      "│   ├── ('13.7', 'Example: node classification')\n",
      "│   │   └── ('13.7.1', 'Choosing batches')\n",
      "│   ├── ('13.8', 'Layers for graph convolutional networks')\n",
      "│   │   ├── ('13.8.1', 'Combining current node and aggregated neighbors')\n",
      "│   │   ├── ('13.8.2', 'Residual connections')\n",
      "│   │   ├── ('13.8.3', 'Mean aggregation')\n",
      "│   │   ├── ('13.8.4', 'Kipf normalization')\n",
      "│   │   ├── ('13.8.5', 'Max pooling aggregation')\n",
      "│   │   └── ('13.8.6', 'Aggregation by attention')\n",
      "│   └── ('13.9', 'Edge graphs')\n",
      "├── ('14', 'Unsupervised learning')\n",
      "│   ├── ('14.1', 'Taxonomy of unsupervised learning models')\n",
      "│   ├── ('14.2', 'What makes a good generative model?')\n",
      "│   ├── ('14.3', 'Quantifying performance')\n",
      "│   └── ('14.4', 'Summary')\n",
      "├── ('15', 'Generative Adversarial Networks')\n",
      "│   ├── ('15.1', 'Discrimination as a signal')\n",
      "│   │   ├── ('15.1.1', 'GAN loss function')\n",
      "│   │   ├── ('15.1.2', 'Training GANs')\n",
      "│   │   ├── ('15.1.3', 'Deep convolutional GAN')\n",
      "│   │   └── ('15.1.4', 'Diﬀiculty training GANs')\n",
      "│   ├── ('15.2', 'Improving stability')\n",
      "│   │   ├── ('15.2.1', 'Analysis of GAN loss function')\n",
      "│   │   ├── ('15.2.2', 'Vanishing gradients')\n",
      "│   │   ├── ('15.2.3', 'Wasserstein distance')\n",
      "│   │   ├── ('15.2.4', 'Wasserstein distance for discrete distributions')\n",
      "│   │   ├── ('15.2.5', 'Wasserstein distance for continuous distributions')\n",
      "│   │   └── ('15.2.6', 'Wasserstein GAN loss function')\n",
      "│   ├── ('15.3', 'Progressive growing, minibatch discrimination, and truncation')\n",
      "│   ├── ('15.4', 'Conditional generation')\n",
      "│   │   ├── ('15.4.1', 'Conditional GAN')\n",
      "│   │   ├── ('15.4.2', 'Auxiliary classifier GAN')\n",
      "│   │   └── ('15.4.3', 'InfoGAN')\n",
      "│   ├── ('15.5', 'Image translation')\n",
      "│   │   ├── ('15.5.1', 'Pix2Pix')\n",
      "│   │   ├── ('15.5.2', 'Adversarial loss')\n",
      "│   │   └── ('15.5.3', 'CycleGAN')\n",
      "│   ├── ('15.6', 'StyleGAN')\n",
      "│   └── ('15.7', 'Summary')\n",
      "├── ('16', 'Normalizing flows')\n",
      "│   ├── ('16.2', 'General case')\n",
      "│   │   ├── ('16.2.1', 'Forward mapping with a deep neural network')\n",
      "│   │   └── ('16.2.2', 'Desiderata for network layers')\n",
      "│   ├── ('16.3', 'Invertible network layers')\n",
      "│   │   ├── ('16.3.1', 'Linear flows')\n",
      "│   │   ├── ('16.3.2', 'Elementwise flows')\n",
      "│   │   ├── ('16.3.3', 'Coupling flows')\n",
      "│   │   ├── ('16.3.4', 'Autoregressive flows')\n",
      "│   │   ├── ('16.3.5', 'Inverse autoregressive flows')\n",
      "│   │   ├── ('16.3.6', 'Residual flows: iRevNet')\n",
      "│   │   └── ('16.3.7', 'Residual flows and contraction mappings: iResNet')\n",
      "│   ├── ('16.4', 'Multi-scale flows')\n",
      "│   ├── ('16.5', 'Applications')\n",
      "│   │   ├── ('16.5.1', 'Modeling densities')\n",
      "│   │   ├── ('16.5.2', 'Synthesis')\n",
      "│   │   └── ('16.5.3', 'Approximating other density models')\n",
      "│   └── ('16.6', 'Summary')\n",
      "├── ('17', 'Variational autoencoders')\n",
      "│   ├── ('17.1', 'Latent variable models')\n",
      "│   │   └── ('17.1.1', 'Example: mixture of Gaussians')\n",
      "│   ├── ('17.2', 'Nonlinear latent variable model')\n",
      "│   │   └── ('17.2.1', 'Generation')\n",
      "│   ├── ('17.3', 'Training')\n",
      "│   │   ├── ('17.3.1', 'Evidence lower bound (ELBO)')\n",
      "│   │   ├── ('17.3.2', 'Jensen’s inequality')\n",
      "│   │   └── ('17.3.3', 'Deriving the bound')\n",
      "│   ├── ('17.4', 'ELBO properties')\n",
      "│   │   ├── ('17.4.1', 'Tightness of bound')\n",
      "│   │   └── ('17.4.2', 'ELBO as reconstruction loss minus KL distance to prior')\n",
      "│   ├── ('17.5', 'Variational approximation')\n",
      "│   ├── ('17.6', 'The variational autoencoder')\n",
      "│   │   └── ('17.6.1', 'VAE algorithm')\n",
      "│   ├── ('17.7', 'The reparameterization trick')\n",
      "│   ├── ('17.8', 'Applications')\n",
      "│   │   ├── ('17.8.1', 'Approximating sample probability')\n",
      "│   │   ├── ('17.8.2', 'Generation')\n",
      "│   │   ├── ('17.8.3', 'Resynthesis')\n",
      "│   │   └── ('17.8.4', 'Disentanglement')\n",
      "│   └── ('17.9', 'Summary')\n",
      "├── ('18', 'Diffusion models')\n",
      "│   ├── ('18.1', 'Overview')\n",
      "│   ├── ('18.2', 'Encoder (forward process)')\n",
      "│   │   ├── ('18.2.1', 'Diffusion kernel q(zt|x)')\n",
      "│   │   ├── ('18.2.2', 'Marginal distributions q(zt)')\n",
      "│   │   ├── ('18.2.3', 'Conditional distribution q(zt−1|zt)')\n",
      "│   │   └── ('18.2.4', 'Conditional diffusion distribution q(zt−1|zt, x)')\n",
      "│   ├── ('18.3', 'Decoder model (reverse process)')\n",
      "│   ├── ('18.4', 'Training')\n",
      "│   │   ├── ('18.4.1', 'Evidence lower bound (ELBO)')\n",
      "│   │   ├── ('18.4.2', 'Simplifying the ELBO')\n",
      "│   │   ├── ('18.4.3', 'Analyzing the ELBO')\n",
      "│   │   ├── ('18.4.4', 'Diffusion loss function')\n",
      "│   │   └── ('18.4.5', 'Training procedure')\n",
      "│   ├── ('18.5', 'Reparameterization of loss function')\n",
      "│   │   ├── ('18.5.1', 'Reparameterization of target')\n",
      "│   │   └── ('18.5.2', 'Reparameterization of network')\n",
      "│   ├── ('18.6', 'Implementation')\n",
      "│   │   ├── ('18.6.1', 'Application to images')\n",
      "│   │   ├── ('18.6.2', 'Improving generation speed')\n",
      "│   │   ├── ('18.6.3', 'Conditional generation')\n",
      "│   │   └── ('18.6.4', 'Improving generation quality')\n",
      "│   └── ('18.7', 'Summary')\n",
      "├── ('19', 'Reinforcement learning')\n",
      "│   ├── ('19.1', 'Markov decision processes, returns, and policies')\n",
      "│   │   ├── ('19.1.1', 'Markov process')\n",
      "│   │   ├── ('19.1.2', 'Markov reward process')\n",
      "│   │   ├── ('19.1.3', 'Markov decision process')\n",
      "│   │   ├── ('19.1.4', 'Partially observable Markov decision process')\n",
      "│   │   └── ('19.1.5', 'Policy')\n",
      "│   ├── ('19.2', 'Expected return')\n",
      "│   │   ├── ('19.2.1', 'State and action values')\n",
      "│   │   ├── ('19.2.2', 'Optimal policy')\n",
      "│   │   └── ('19.2.3', 'Bellman equations')\n",
      "│   ├── ('19.3', 'Tabular reinforcement learning')\n",
      "│   │   ├── ('19.3.1', 'Dynamic programming')\n",
      "│   │   ├── ('19.3.2', 'Monte Carlo methods')\n",
      "│   │   └── ('19.3.3', 'Temporal difference methods')\n",
      "│   ├── ('19.4', 'Fitted Q-learning')\n",
      "│   │   ├── ('19.4.1', 'Deep Q-networks for playing ATARI games')\n",
      "│   │   └── ('19.4.2', 'Double Q-learning and double deep Q-networks')\n",
      "│   ├── ('19.5', 'Policy gradient methods')\n",
      "│   │   ├── ('19.5.1', 'Derivation of gradient update')\n",
      "│   │   ├── ('19.5.2', 'REINFORCE algorithm')\n",
      "│   │   ├── ('19.5.3', 'Baselines')\n",
      "│   │   └── ('19.5.4', 'State-dependent baselines')\n",
      "│   ├── ('19.6', 'Actor-critic methods')\n",
      "│   ├── ('19.7', 'Offline reinforcement learning')\n",
      "│   └── ('19.8', 'Summary')\n",
      "├── ('20', 'Why does deep learning work?')\n",
      "│   ├── ('20.1', 'The case against deep learning')\n",
      "│   │   ├── ('20.1.1', 'Training')\n",
      "│   │   ├── ('20.1.2', 'Generalization')\n",
      "│   │   └── ('20.1.3', 'The unreasonable effectiveness of deep learning')\n",
      "│   ├── ('20.2', 'Factors that influence fitting performance')\n",
      "│   │   ├── ('20.2.1', 'Dataset')\n",
      "│   │   ├── ('20.2.2', 'Regularization')\n",
      "│   │   ├── ('20.2.3', 'Stochastic training algorithms')\n",
      "│   │   ├── ('20.2.4', 'Overparameterization')\n",
      "│   │   ├── ('20.2.5', 'Activation functions')\n",
      "│   │   ├── ('20.2.6', 'Initialization')\n",
      "│   │   └── ('20.2.7', 'Network depth')\n",
      "│   ├── ('20.3', 'Properties of loss functions')\n",
      "│   │   ├── ('20.3.1', 'Multiple global minima')\n",
      "│   │   ├── ('20.3.2', 'Route to the minimum')\n",
      "│   │   ├── ('20.3.3', 'Connections between minima')\n",
      "│   │   └── ('20.3.4', 'Curvature of loss surface')\n",
      "│   ├── ('20.4', 'Factors that determine generalization')\n",
      "│   │   ├── ('20.4.1', 'Training algorithms')\n",
      "│   │   ├── ('20.4.2', 'Flatness of minimum')\n",
      "│   │   ├── ('20.4.3', 'Architecture')\n",
      "│   │   ├── ('20.4.4', 'Norm of weights')\n",
      "│   │   ├── ('20.4.5', 'Overparameterization')\n",
      "│   │   └── ('20.4.6', 'Leaving the data manifold')\n",
      "│   ├── ('20.5', 'Do we need so many parameters?')\n",
      "│   │   ├── ('20.5.1', 'Pruning')\n",
      "│   │   ├── ('20.5.2', 'Knowledge distillation')\n",
      "│   │   └── ('20.5.3', 'Discussion')\n",
      "│   ├── ('20.6', 'Do networks have to be deep?')\n",
      "│   │   ├── ('20.6.1', 'Complexity of modeled function')\n",
      "│   │   ├── ('20.6.2', 'Tractability of training')\n",
      "│   │   └── ('20.6.3', 'Inductive bias')\n",
      "│   └── ('20.7', 'Summary')\n",
      "└── ('21', 'Deep learning and ethics')\n",
      "    ├── ('21.1', 'Value alignment')\n",
      "    │   ├── ('21.1.1', 'Bias and fairness')\n",
      "    │   ├── ('21.1.2', 'Artificial moral agency')\n",
      "    │   ├── ('21.1.3', 'Transparency and opacity')\n",
      "    │   └── ('21.1.4', 'Explainability and interpretability')\n",
      "    ├── ('21.2', 'Intentional misuse')\n",
      "    │   ├── ('21.2.1', 'Face recognition and analysis')\n",
      "    │   ├── ('21.2.2', 'Militarization and political interference')\n",
      "    │   ├── ('21.2.3', 'Fraud')\n",
      "    │   └── ('21.2.4', 'Data privacy')\n",
      "    ├── ('21.3', 'Other social, ethical, and professional issues')\n",
      "    │   ├── ('21.3.1', 'Intellectual property')\n",
      "    │   ├── ('21.3.2', 'Automation bias and moral deskilling')\n",
      "    │   ├── ('21.3.3', 'Environmental impact')\n",
      "    │   ├── ('21.3.4', 'Employment and society')\n",
      "    │   └── ('21.3.5', 'Concentration of power')\n",
      "    ├── ('21.4', 'Case study')\n",
      "    ├── ('21.5', 'The value-free ideal of science')\n",
      "    ├── ('21.6', 'Responsible AI research as a collective action problem')\n",
      "    │   ├── ('21.6.1', 'Scientific communication')\n",
      "    │   └── ('21.6.2', 'Diversity and heterogeneity')\n",
      "    ├── ('21.7', 'Ways forward')\n",
      "    └── ('21.8', 'Summary')\n"
     ]
    }
   ],
   "source": [
    "display_tree(htree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database and table for storing the hierarchical index\n",
    "conn = sqlite3.connect('db//UnderstandingDeepLearning.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE IF NOT EXISTS understanding_deep_learning (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    parent_id INTEGER,\n",
    "    identifier TEXT,\n",
    "    title TEXT,\n",
    "    text TEXT,\n",
    "    FOREIGN KEY (parent_id) REFERENCES understanding_deep_learning (id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Recursively insert nodes into the database\n",
    "def tree_to_db(node, parent_id=None):\n",
    "    identifier = getattr(node, 'identifier', None)\n",
    "    title = node.name[1]\n",
    "    text = getattr(node, 'text', None)\n",
    "    c.execute('INSERT INTO understanding_deep_learning (parent_id, identifier, title, text) VALUES (?, ?, ?, ?)', \n",
    "              (parent_id, identifier, title, text))\n",
    "    node_id = c.lastrowid\n",
    "    for child in node.children:\n",
    "        tree_to_db(child, node_id)\n",
    "\n",
    "tree_to_db(htree)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
