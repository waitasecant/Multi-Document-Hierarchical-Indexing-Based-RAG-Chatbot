{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from anytree import Node, RenderTree\n",
    "import re\n",
    "import sqlite3\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(path):\n",
    "    doc = fitz.open(path)\n",
    "    text = \"\"\n",
    "    for i in range(len(doc)):\n",
    "        page = doc.load_page(i)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "def extract_chapter(i):\n",
    "    x = i.split('\\n')\n",
    "    return x[1].split(' ')[-1], x[-1]\n",
    "\n",
    "def display_tree(root):\n",
    "    for pre, _, node in RenderTree(root):\n",
    "        print(\"%s%s\" % (pre, node.name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data//UnderstandingDeepLearning.pdf\"\n",
    "\n",
    "pattern_chap = r'\\nChapter \\d+\\n[^\\n]*'\n",
    "pattern_sec = r'\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])'\n",
    "pattern_seubsec = r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*'\n",
    "pattern_sec_subsec = re.compile(r'\\n\\d+\\.\\d+\\.\\d\\n[^\\n]*|\\n\\d+\\.\\d\\n\\D+\\n(?![\\d\\.])')\n",
    "\n",
    "matches_chap = []\n",
    "matches_sec = []\n",
    "matches_subsec = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = extract_text(path)\n",
    "\n",
    "matches_chap.extend(re.findall(pattern_chap, text))\n",
    "matches_chap = [extract_chapter(i) for i in matches_chap]\n",
    "\n",
    "matches_sec.extend(re.findall(pattern_sec, text))\n",
    "matches_sec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_sec]\n",
    "\n",
    "matches_subsec.extend(re.findall(pattern_seubsec, text))\n",
    "matches_subsec = [(i.split('\\n')[1],i.split('\\n')[2]) for i in matches_subsec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_index():\n",
    "\n",
    "    root = Node(\"Understanding Deep Learning\")\n",
    "\n",
    "    # Joining chapters to root\n",
    "    for i in matches_chap:\n",
    "        Node(i[1], parent=root, identifier=i[0], text=None)\n",
    "\n",
    "    # Joining sections to chapters\n",
    "    for i in matches_sec:\n",
    "        for j in [(chap.identifier, chap) for chap in root.children]:\n",
    "            if i[0].split('.')[0] == j[0]:\n",
    "                Node(i[1], parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Joining subsections to sections\n",
    "    for i in matches_subsec:\n",
    "        all_sections = [i.children for i in root.children]\n",
    "        all_sections = [node for sublist in all_sections for node in sublist]\n",
    "        all_sections = [(sec.identifier,sec) for sec in all_sections]\n",
    "\n",
    "        for j in all_sections:\n",
    "            if '.'.join(i[0].split('.')[:-1]) == j[0]:\n",
    "                Node(i[1], parent=j[1], identifier=i[0], text=None)\n",
    "\n",
    "    # Adding text between sections and subsections to subsections node\n",
    "    matches = list(re.finditer(pattern_sec_subsec, text))\n",
    "\n",
    "    for i in range(len(matches) - 1):\n",
    "        start = matches[i].end()\n",
    "        end = matches[i + 1].start()\n",
    "        subsection_num = matches[i].group().split('\\n')[1]\n",
    "        between_text = text[start:end]\n",
    "        between_text = ' '.join(between_text.split('\\n'))\n",
    "\n",
    "        leaves = [(l.identifier,l) for l in root.leaves]\n",
    "        for j in leaves:\n",
    "            if subsection_num == j[0]:\n",
    "                j[1].text = between_text\n",
    "    if matches:\n",
    "        between_text = ' '.join(text[matches[-1].start():matches[-1].start()+1000].split('\\n'))\n",
    "        root.leaves[-1].text = between_text\n",
    "        \n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "htree = hierarchical_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_tree(htree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLite database and table for storing the hierarchical index\n",
    "conn = sqlite3.connect('db//UnderstandingDeepLearning.db')\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''\n",
    "CREATE TABLE IF NOT EXISTS understanding_deep_learning (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    parent_id INTEGER,\n",
    "    identifier TEXT,\n",
    "    title TEXT,\n",
    "    text TEXT,\n",
    "    FOREIGN KEY (parent_id) REFERENCES understanding_deep_learning (id)\n",
    ")\n",
    "''')\n",
    "\n",
    "# Recursively insert nodes into the database\n",
    "def tree_to_db(node, parent_id=None):\n",
    "    identifier = getattr(node, 'identifier', None)\n",
    "    title = node.name\n",
    "    text = getattr(node, 'text', None)\n",
    "    c.execute('INSERT INTO understanding_deep_learning (parent_id, identifier, title, text) VALUES (?, ?, ?, ?)', \n",
    "              (parent_id, identifier, title, text))\n",
    "    node_id = c.lastrowid\n",
    "    for child in node.children:\n",
    "        tree_to_db(child, node_id)\n",
    "\n",
    "tree_to_db(htree)\n",
    "\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_query(query):\n",
    "    words = simple_preprocess(query)\n",
    "    expanded_query = set(words)\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    for word in words:\n",
    "        # Add synonyms\n",
    "        synonyms = wordnet.synsets(word)\n",
    "        for syn in synonyms:\n",
    "            for lemma in syn.lemmas():\n",
    "                expanded_query.add(lemma.name())\n",
    "        \n",
    "        # Add stemmed words\n",
    "        expanded_query.add(stemmer.stem(word))\n",
    "\n",
    "    return list(expanded_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['deep', 'get_a_line', 'cryptic', 'mystifying', 'acquisition', 'cryptical', 'acquire', 'rich', 'ascertain', 'learn', 'hear', 'memorise', 'teach', 'bass', 'encyclopaedism', 'scholarship', 'larn', 'trench', 'get_wind', 'mysterious', 'check', 'encyclopedism', 'find_out', 'abstruse', 'study', 'see', 'instruct', 'con', 'get_word', 'inscrutable', 'discover', 'thick', 'late', 'erudition', 'watch', 'learning', 'eruditeness', 'pick_up', 'read', 'recondite', 'memorize', 'take', 'determine', 'learnedness', 'oceanic_abyss', 'deeply']\n"
     ]
    }
   ],
   "source": [
    "query = \"deep learning\"\n",
    "expanded_query = expand_query(query)\n",
    "print(expanded_query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
